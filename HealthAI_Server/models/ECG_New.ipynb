{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyZxIaklIeMj",
        "outputId": "ebc1b65c-808b-454d-cf5b-7190171260fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.12/dist-packages (4.22.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.2)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.10.7)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.31)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.80)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.10)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.11.12)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.15.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.13.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.1.4)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (24.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.14.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (5.9.5)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.23.0)\n",
            "â¬‡ï¸ Äang táº£i dá»¯ liá»‡u MIT-BIH...\n",
            "Generating record list for: 100\n",
            "Generating record list for: 101\n",
            "Generating record list for: 102\n",
            "Generating record list for: 103\n",
            "Generating record list for: 104\n",
            "Generating record list for: 105\n",
            "Generating record list for: 106\n",
            "Generating record list for: 107\n",
            "Generating record list for: 108\n",
            "Generating record list for: 109\n",
            "Generating record list for: 111\n",
            "Generating record list for: 112\n",
            "Generating record list for: 113\n",
            "Generating record list for: 114\n",
            "Generating record list for: 115\n",
            "Generating record list for: 116\n",
            "Generating record list for: 117\n",
            "Generating record list for: 118\n",
            "Generating record list for: 119\n",
            "Generating record list for: 121\n",
            "Generating record list for: 122\n",
            "Generating record list for: 123\n",
            "Generating record list for: 124\n",
            "Generating record list for: 200\n",
            "Generating record list for: 201\n",
            "Generating record list for: 202\n",
            "Generating record list for: 203\n",
            "Generating record list for: 205\n",
            "Generating record list for: 207\n",
            "Generating record list for: 208\n",
            "Generating record list for: 209\n",
            "Generating record list for: 210\n",
            "Generating record list for: 212\n",
            "Generating record list for: 213\n",
            "Generating record list for: 214\n",
            "Generating record list for: 215\n",
            "Generating record list for: 217\n",
            "Generating record list for: 219\n",
            "Generating record list for: 220\n",
            "Generating record list for: 221\n",
            "Generating record list for: 222\n",
            "Generating record list for: 223\n",
            "Generating record list for: 228\n",
            "Generating record list for: 230\n",
            "Generating record list for: 231\n",
            "Generating record list for: 232\n",
            "Generating record list for: 233\n",
            "Generating record list for: 234\n",
            "Generating list of all files for: 100\n",
            "Generating list of all files for: 101\n",
            "Generating list of all files for: 102\n",
            "Generating list of all files for: 103\n",
            "Generating list of all files for: 104\n",
            "Generating list of all files for: 105\n",
            "Generating list of all files for: 106\n",
            "Generating list of all files for: 107\n",
            "Generating list of all files for: 108\n",
            "Generating list of all files for: 109\n",
            "Generating list of all files for: 111\n",
            "Generating list of all files for: 112\n",
            "Generating list of all files for: 113\n",
            "Generating list of all files for: 114\n",
            "Generating list of all files for: 115\n",
            "Generating list of all files for: 116\n",
            "Generating list of all files for: 117\n",
            "Generating list of all files for: 118\n",
            "Generating list of all files for: 119\n",
            "Generating list of all files for: 121\n",
            "Generating list of all files for: 122\n",
            "Generating list of all files for: 123\n",
            "Generating list of all files for: 124\n",
            "Generating list of all files for: 200\n",
            "Generating list of all files for: 201\n",
            "Generating list of all files for: 202\n",
            "Generating list of all files for: 203\n",
            "Generating list of all files for: 205\n",
            "Generating list of all files for: 207\n",
            "Generating list of all files for: 208\n",
            "Generating list of all files for: 209\n",
            "Generating list of all files for: 210\n",
            "Generating list of all files for: 212\n",
            "Generating list of all files for: 213\n",
            "Generating list of all files for: 214\n",
            "Generating list of all files for: 215\n",
            "Generating list of all files for: 217\n",
            "Generating list of all files for: 219\n",
            "Generating list of all files for: 220\n",
            "Generating list of all files for: 221\n",
            "Generating list of all files for: 222\n",
            "Generating list of all files for: 223\n",
            "Generating list of all files for: 228\n",
            "Generating list of all files for: 230\n",
            "Generating list of all files for: 231\n",
            "Generating list of all files for: 232\n",
            "Generating list of all files for: 233\n",
            "Generating list of all files for: 234\n",
            "Created local base download directory: mitdb_data\n",
            "Downloading files...\n",
            "Finished downloading files\n",
            "âš™ï¸ Äang xá»­ lÃ½ tÃ­n hiá»‡u & Augmentation...\n",
            "\n",
            "ğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u sau Augmentation:\n",
            " - Class N: 90084 máº«u\n",
            " - Class S: 8343 máº«u\n",
            " - Class V: 21024 máº«u\n",
            " - Class F: 2406 máº«u\n",
            "\n",
            "ğŸ“ Global Scaler: Mean=-0.2877, Std=0.5242\n",
            "\n",
            "ğŸš€ Báº¯t Ä‘áº§u Training...\n",
            "Epoch 1/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7765 - loss: 0.7932\n",
            "Epoch 1: val_accuracy improved from -inf to 0.79053, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 75ms/step - accuracy: 0.7767 - loss: 0.7929 - val_accuracy: 0.7905 - val_loss: 0.7857 - learning_rate: 0.0010\n",
            "Epoch 2/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9364 - loss: 0.5176\n",
            "Epoch 2: val_accuracy improved from 0.79053 to 0.95049, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9365 - loss: 0.5175 - val_accuracy: 0.9505 - val_loss: 0.4695 - learning_rate: 9.9932e-04\n",
            "Epoch 3/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9578 - loss: 0.4592\n",
            "Epoch 3: val_accuracy improved from 0.95049 to 0.95773, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9578 - loss: 0.4591 - val_accuracy: 0.9577 - val_loss: 0.4561 - learning_rate: 9.9729e-04\n",
            "Epoch 4/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9633 - loss: 0.4488\n",
            "Epoch 4: val_accuracy improved from 0.95773 to 0.97144, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9633 - loss: 0.4488 - val_accuracy: 0.9714 - val_loss: 0.4254 - learning_rate: 9.9391e-04\n",
            "Epoch 5/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9707 - loss: 0.4212\n",
            "Epoch 5: val_accuracy did not improve from 0.97144\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9707 - loss: 0.4211 - val_accuracy: 0.9292 - val_loss: 0.5156 - learning_rate: 9.8918e-04\n",
            "Epoch 6/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9743 - loss: 0.4125\n",
            "Epoch 6: val_accuracy improved from 0.97144 to 0.97588, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9743 - loss: 0.4125 - val_accuracy: 0.9759 - val_loss: 0.4110 - learning_rate: 9.8313e-04\n",
            "Epoch 7/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9756 - loss: 0.4112\n",
            "Epoch 7: val_accuracy did not improve from 0.97588\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9756 - loss: 0.4112 - val_accuracy: 0.9591 - val_loss: 0.4488 - learning_rate: 9.7577e-04\n",
            "Epoch 8/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9811 - loss: 0.3951\n",
            "Epoch 8: val_accuracy improved from 0.97588 to 0.97816, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9811 - loss: 0.3951 - val_accuracy: 0.9782 - val_loss: 0.4077 - learning_rate: 9.6712e-04\n",
            "Epoch 9/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9819 - loss: 0.3970\n",
            "Epoch 9: val_accuracy improved from 0.97816 to 0.98210, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9819 - loss: 0.3970 - val_accuracy: 0.9821 - val_loss: 0.3915 - learning_rate: 9.5721e-04\n",
            "Epoch 10/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9851 - loss: 0.3883\n",
            "Epoch 10: val_accuracy did not improve from 0.98210\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9851 - loss: 0.3883 - val_accuracy: 0.9655 - val_loss: 0.4225 - learning_rate: 9.4605e-04\n",
            "Epoch 11/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9857 - loss: 0.3853\n",
            "Epoch 11: val_accuracy did not improve from 0.98210\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9857 - loss: 0.3853 - val_accuracy: 0.9731 - val_loss: 0.4073 - learning_rate: 9.3368e-04\n",
            "Epoch 12/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9864 - loss: 0.3796\n",
            "Epoch 12: val_accuracy did not improve from 0.98210\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9864 - loss: 0.3796 - val_accuracy: 0.9777 - val_loss: 0.3994 - learning_rate: 9.2014e-04\n",
            "Epoch 13/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9880 - loss: 0.3788\n",
            "Epoch 13: val_accuracy did not improve from 0.98210\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9880 - loss: 0.3789 - val_accuracy: 0.9629 - val_loss: 0.4159 - learning_rate: 9.0546e-04\n",
            "Epoch 14/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9823 - loss: 0.3901\n",
            "Epoch 14: val_accuracy did not improve from 0.98210\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9824 - loss: 0.3900 - val_accuracy: 0.9811 - val_loss: 0.3904 - learning_rate: 8.8969e-04\n",
            "Epoch 15/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9901 - loss: 0.3713\n",
            "Epoch 15: val_accuracy improved from 0.98210 to 0.98515, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.9901 - loss: 0.3713 - val_accuracy: 0.9851 - val_loss: 0.3821 - learning_rate: 8.7286e-04\n",
            "Epoch 16/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9912 - loss: 0.3725\n",
            "Epoch 16: val_accuracy did not improve from 0.98515\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9912 - loss: 0.3725 - val_accuracy: 0.9831 - val_loss: 0.3843 - learning_rate: 8.5502e-04\n",
            "Epoch 17/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9913 - loss: 0.3758\n",
            "Epoch 17: val_accuracy improved from 0.98515 to 0.98553, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9913 - loss: 0.3758 - val_accuracy: 0.9855 - val_loss: 0.3797 - learning_rate: 8.3622e-04\n",
            "Epoch 18/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9917 - loss: 0.3728\n",
            "Epoch 18: val_accuracy did not improve from 0.98553\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9917 - loss: 0.3728 - val_accuracy: 0.9853 - val_loss: 0.3820 - learning_rate: 8.1651e-04\n",
            "Epoch 19/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9918 - loss: 0.3705\n",
            "Epoch 19: val_accuracy did not improve from 0.98553\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9918 - loss: 0.3705 - val_accuracy: 0.9843 - val_loss: 0.3856 - learning_rate: 7.9595e-04\n",
            "Epoch 20/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9940 - loss: 0.3719\n",
            "Epoch 20: val_accuracy did not improve from 0.98553\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9940 - loss: 0.3719 - val_accuracy: 0.9851 - val_loss: 0.3816 - learning_rate: 7.7460e-04\n",
            "Epoch 21/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9945 - loss: 0.3653\n",
            "Epoch 21: val_accuracy did not improve from 0.98553\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9945 - loss: 0.3653 - val_accuracy: 0.9834 - val_loss: 0.3823 - learning_rate: 7.5250e-04\n",
            "Epoch 22/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9949 - loss: 0.3666\n",
            "Epoch 22: val_accuracy improved from 0.98553 to 0.98730, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9949 - loss: 0.3666 - val_accuracy: 0.9873 - val_loss: 0.3781 - learning_rate: 7.2973e-04\n",
            "Epoch 23/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9948 - loss: 0.3671\n",
            "Epoch 23: val_accuracy did not improve from 0.98730\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9948 - loss: 0.3671 - val_accuracy: 0.9820 - val_loss: 0.3853 - learning_rate: 7.0633e-04\n",
            "Epoch 24/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9940 - loss: 0.3671\n",
            "Epoch 24: val_accuracy improved from 0.98730 to 0.98972, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9940 - loss: 0.3671 - val_accuracy: 0.9897 - val_loss: 0.3721 - learning_rate: 6.8239e-04\n",
            "Epoch 25/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9949 - loss: 0.3637\n",
            "Epoch 25: val_accuracy did not improve from 0.98972\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9949 - loss: 0.3637 - val_accuracy: 0.9858 - val_loss: 0.3795 - learning_rate: 6.5796e-04\n",
            "Epoch 26/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9933 - loss: 0.3650\n",
            "Epoch 26: val_accuracy did not improve from 0.98972\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9933 - loss: 0.3650 - val_accuracy: 0.9882 - val_loss: 0.3737 - learning_rate: 6.3312e-04\n",
            "Epoch 27/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9951 - loss: 0.3614\n",
            "Epoch 27: val_accuracy did not improve from 0.98972\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9951 - loss: 0.3614 - val_accuracy: 0.9883 - val_loss: 0.3731 - learning_rate: 6.0792e-04\n",
            "Epoch 28/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9962 - loss: 0.3618\n",
            "Epoch 28: val_accuracy improved from 0.98972 to 0.98997, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9962 - loss: 0.3618 - val_accuracy: 0.9900 - val_loss: 0.3720 - learning_rate: 5.8244e-04\n",
            "Epoch 29/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9967 - loss: 0.3554\n",
            "Epoch 29: val_accuracy did not improve from 0.98997\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9967 - loss: 0.3555 - val_accuracy: 0.9840 - val_loss: 0.3846 - learning_rate: 5.5674e-04\n",
            "Epoch 30/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9959 - loss: 0.3584\n",
            "Epoch 30: val_accuracy did not improve from 0.98997\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9959 - loss: 0.3584 - val_accuracy: 0.9883 - val_loss: 0.3745 - learning_rate: 5.3091e-04\n",
            "Epoch 31/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9966 - loss: 0.3615\n",
            "Epoch 31: val_accuracy did not improve from 0.98997\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9966 - loss: 0.3615 - val_accuracy: 0.9893 - val_loss: 0.3712 - learning_rate: 5.0500e-04\n",
            "Epoch 32/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.3581\n",
            "Epoch 32: val_accuracy improved from 0.98997 to 0.99149, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.3581 - val_accuracy: 0.9915 - val_loss: 0.3675 - learning_rate: 4.7909e-04\n",
            "Epoch 33/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.3564\n",
            "Epoch 33: val_accuracy did not improve from 0.99149\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.3564 - val_accuracy: 0.9897 - val_loss: 0.3712 - learning_rate: 4.5326e-04\n",
            "Epoch 34/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9979 - loss: 0.3575\n",
            "Epoch 34: val_accuracy did not improve from 0.99149\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9979 - loss: 0.3575 - val_accuracy: 0.9912 - val_loss: 0.3703 - learning_rate: 4.2756e-04\n",
            "Epoch 35/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9981 - loss: 0.3536\n",
            "Epoch 35: val_accuracy did not improve from 0.99149\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9981 - loss: 0.3536 - val_accuracy: 0.9905 - val_loss: 0.3711 - learning_rate: 4.0208e-04\n",
            "Epoch 36/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.3536\n",
            "Epoch 36: val_accuracy did not improve from 0.99149\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.3536 - val_accuracy: 0.9910 - val_loss: 0.3674 - learning_rate: 3.7688e-04\n",
            "Epoch 37/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9993 - loss: 0.3567\n",
            "Epoch 37: val_accuracy did not improve from 0.99149\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9993 - loss: 0.3567 - val_accuracy: 0.9826 - val_loss: 0.3871 - learning_rate: 3.5204e-04\n",
            "Epoch 38/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.3552\n",
            "Epoch 38: val_accuracy did not improve from 0.99149\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9980 - loss: 0.3552 - val_accuracy: 0.9905 - val_loss: 0.3682 - learning_rate: 3.2761e-04\n",
            "Epoch 39/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9994 - loss: 0.3532\n",
            "Epoch 39: val_accuracy improved from 0.99149 to 0.99213, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9994 - loss: 0.3532 - val_accuracy: 0.9921 - val_loss: 0.3668 - learning_rate: 3.0367e-04\n",
            "Epoch 40/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9995 - loss: 0.3548\n",
            "Epoch 40: val_accuracy did not improve from 0.99213\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9995 - loss: 0.3548 - val_accuracy: 0.9911 - val_loss: 0.3670 - learning_rate: 2.8027e-04\n",
            "Epoch 41/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 0.3548\n",
            "Epoch 41: val_accuracy improved from 0.99213 to 0.99226, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.9997 - loss: 0.3548 - val_accuracy: 0.9923 - val_loss: 0.3656 - learning_rate: 2.5750e-04\n",
            "Epoch 42/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9996 - loss: 0.3508\n",
            "Epoch 42: val_accuracy improved from 0.99226 to 0.99238, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9996 - loss: 0.3509 - val_accuracy: 0.9924 - val_loss: 0.3654 - learning_rate: 2.3540e-04\n",
            "Epoch 43/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 0.3525\n",
            "Epoch 43: val_accuracy did not improve from 0.99238\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 0.3525 - val_accuracy: 0.9920 - val_loss: 0.3660 - learning_rate: 2.1405e-04\n",
            "Epoch 44/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9998 - loss: 0.3515\n",
            "Epoch 44: val_accuracy did not improve from 0.99238\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 0.3515 - val_accuracy: 0.9924 - val_loss: 0.3650 - learning_rate: 1.9349e-04\n",
            "Epoch 45/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3536\n",
            "Epoch 45: val_accuracy improved from 0.99238 to 0.99302, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3536 - val_accuracy: 0.9930 - val_loss: 0.3640 - learning_rate: 1.7378e-04\n",
            "Epoch 46/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3540\n",
            "Epoch 46: val_accuracy did not improve from 0.99302\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3540 - val_accuracy: 0.9930 - val_loss: 0.3647 - learning_rate: 1.5498e-04\n",
            "Epoch 47/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9999 - loss: 0.3542\n",
            "Epoch 47: val_accuracy did not improve from 0.99302\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 0.3542 - val_accuracy: 0.9916 - val_loss: 0.3683 - learning_rate: 1.3714e-04\n",
            "Epoch 48/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 0.3542\n",
            "Epoch 48: val_accuracy did not improve from 0.99302\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9999 - loss: 0.3542 - val_accuracy: 0.9929 - val_loss: 0.3645 - learning_rate: 1.2031e-04\n",
            "Epoch 49/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 0.3545\n",
            "Epoch 49: val_accuracy did not improve from 0.99302\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9999 - loss: 0.3545 - val_accuracy: 0.9930 - val_loss: 0.3642 - learning_rate: 1.0454e-04\n",
            "Epoch 50/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3518\n",
            "Epoch 50: val_accuracy improved from 0.99302 to 0.99327, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3518 - val_accuracy: 0.9933 - val_loss: 0.3645 - learning_rate: 8.9858e-05\n",
            "Epoch 51/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3534\n",
            "Epoch 51: val_accuracy did not improve from 0.99327\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3534 - val_accuracy: 0.9929 - val_loss: 0.3637 - learning_rate: 7.6317e-05\n",
            "Epoch 52/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3530\n",
            "Epoch 52: val_accuracy did not improve from 0.99327\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3530 - val_accuracy: 0.9931 - val_loss: 0.3643 - learning_rate: 6.3952e-05\n",
            "Epoch 53/60\n",
            "\u001b[1m347/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3541\n",
            "Epoch 53: val_accuracy improved from 0.99327 to 0.99353, saving model to best_ultra_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3541 - val_accuracy: 0.9935 - val_loss: 0.3640 - learning_rate: 5.2795e-05\n",
            "Epoch 54/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3502\n",
            "Epoch 54: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3502 - val_accuracy: 0.9928 - val_loss: 0.3642 - learning_rate: 4.2878e-05\n",
            "Epoch 55/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3516\n",
            "Epoch 55: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3516 - val_accuracy: 0.9928 - val_loss: 0.3644 - learning_rate: 3.4227e-05\n",
            "Epoch 56/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3540\n",
            "Epoch 56: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3540 - val_accuracy: 0.9934 - val_loss: 0.3641 - learning_rate: 2.6867e-05\n",
            "Epoch 57/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3526\n",
            "Epoch 57: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3526 - val_accuracy: 0.9929 - val_loss: 0.3641 - learning_rate: 2.0817e-05\n",
            "Epoch 58/60\n",
            "\u001b[1m348/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3519\n",
            "Epoch 58: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.3519 - val_accuracy: 0.9930 - val_loss: 0.3642 - learning_rate: 1.6094e-05\n",
            "Epoch 59/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3526\n",
            "Epoch 59: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3526 - val_accuracy: 0.9930 - val_loss: 0.3642 - learning_rate: 1.2712e-05\n",
            "Epoch 60/60\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9999 - loss: 0.3503\n",
            "Epoch 60: val_accuracy did not improve from 0.99353\n",
            "\u001b[1m349/349\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9999 - loss: 0.3503 - val_accuracy: 0.9933 - val_loss: 0.3640 - learning_rate: 1.0678e-05\n",
            "\n",
            "ğŸ† ÄÃ¡nh giÃ¡ trÃªn táº­p Test (Dá»¯ liá»‡u chÆ°a tá»«ng há»c):\n",
            "\u001b[1m290/290\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           N     0.9953    0.9960    0.9957      4500\n",
            "           S     0.9920    0.9856    0.9888      1251\n",
            "           V     0.9972    0.9990    0.9981      3154\n",
            "           F     0.9889    0.9861    0.9875       361\n",
            "\n",
            "    accuracy                         0.9953      9266\n",
            "   macro avg     0.9933    0.9917    0.9925      9266\n",
            "weighted avg     0.9952    0.9953    0.9952      9266\n",
            "\n",
            "\n",
            "ğŸ“¦ Äang Ä‘Ã³ng gÃ³i model...\n",
            "Saved artifact at 'saved_model_ecg'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 1), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134722910038032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910031504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910033808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910037072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910041296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910038608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910043984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910032272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910037264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910035920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910039760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910042640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910034192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910029392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910036880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910037840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910039568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910033232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910040912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910039184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910036496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910043792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910034960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910041872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910036304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910030736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910039952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910041104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910040144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910043024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910043408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910030352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910034384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910030544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910028816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910040336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910031696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910041680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503033680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503034640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503034448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503033872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503036368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503036176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503035024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503034832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503033104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503035984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503035792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503035216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503033488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503037328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503037136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503036560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503037712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503037520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910042256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910029584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910043600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910041488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722910038992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503032912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503035600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503038864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503038672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503037904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503038288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503034064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503036944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503040016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503039824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503033296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503040976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503038480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503040400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503040208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503039248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503041360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503041168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503040592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503039440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503042512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503042320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503039056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503042128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503041552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503041744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134722503043472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "2026-01-04 15:32:21.658291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767540741.681390    5110 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767540741.688192    5110 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767540741.705934    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767540741.705964    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767540741.705969    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767540741.705973    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
            "2026-01-04 15:32:30.418909: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1767540750.419106    5110 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6138 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "I0000 00:00:1767540751.281609    5110 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1767540751.281789    5110 single_machine.cc:374] Starting new session\n",
            "I0000 00:00:1767540751.282812    5110 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6138 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "  adding: tfjs_model/ (stored 0%)\n",
            "  adding: tfjs_model/group1-shard3of3.bin (deflated 7%)\n",
            "  adding: tfjs_model/model.json (deflated 95%)\n",
            "  adding: tfjs_model/group1-shard1of3.bin (deflated 7%)\n",
            "  adding: tfjs_model/group1-shard2of3.bin (deflated 7%)\n",
            "  adding: scaler_ecg.json (deflated 2%)\n",
            "\n",
            "âœ… HOÃ€N Táº¤T! HÃ£y táº£i file 'tfjs_model_ultra.zip' vá».\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# BÆ¯á»šC 0: Dá»ŒN Dáº¸P Sáº CH Sáº¼ & CÃ€I Äáº¶T\n",
        "# ==============================================================================\n",
        "# XÃ³a cÃ¡c file/folder cÅ© Ä‘á»ƒ trÃ¡nh nháº§m láº«n\n",
        "!rm -rf tfjs_model tfjs_model_ultra.zip scaler_ecg.json *.h5\n",
        "\n",
        "# CÃ i Ä‘áº·t thÆ° viá»‡n\n",
        "!pip install wfdb tensorflowjs scikit-learn matplotlib numpy pandas\n",
        "\n",
        "import wfdb\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from scipy.signal import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Conv1D, MaxPooling1D, Dense, Flatten, Dropout,\n",
        "                                     BatchNormalization, Input, Add, Activation,\n",
        "                                     GlobalAveragePooling1D, SpatialDropout1D)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ==============================================================================\n",
        "# BÆ¯á»šC 1: Cáº¤U HÃŒNH Há»† THá»NG\n",
        "# ==============================================================================\n",
        "TARGET_FS = 125        # Táº§n sá»‘ láº¥y máº«u cá»§a AD8232\n",
        "WINDOW_SIZE = 100      # 100 máº«u (0.8 giÃ¢y)\n",
        "SHIFT_RANGE = 4        # Dá»‹ch chuyá»ƒn cá»­a sá»• +/- 4 máº«u Ä‘á»ƒ táº¡o thÃªm dá»¯ liá»‡u\n",
        "\n",
        "# ==============================================================================\n",
        "# BÆ¯á»šC 2: Táº¢I & Xá»¬ LÃ Dá»® LIá»†U (SHIFT AUGMENTATION)\n",
        "# ==============================================================================\n",
        "print(\"â¬‡ï¸ Äang táº£i dá»¯ liá»‡u MIT-BIH...\")\n",
        "wfdb.dl_database('mitdb', dl_dir='mitdb_data')\n",
        "\n",
        "classes = ['N', 'S', 'V', 'F']\n",
        "aami_dict = {\n",
        "    'N': 'N', 'L': 'N', 'R': 'N', 'e': 'N', 'j': 'N',\n",
        "    'A': 'S', 'a': 'S', 'J': 'S', 'S': 'S',\n",
        "    'V': 'V', 'E': 'V', 'F': 'F', '/': 'Q', 'f': 'Q', 'Q': 'Q'\n",
        "}\n",
        "class_map = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "data_by_class = {0: [], 1: [], 2: [], 3: []}\n",
        "records = [f.split('.')[0] for f in os.listdir('mitdb_data') if f.endswith('.dat')]\n",
        "records = sorted(list(set(records)))\n",
        "\n",
        "print(\"âš™ï¸ Äang xá»­ lÃ½ tÃ­n hiá»‡u & Augmentation...\")\n",
        "for record_name in records:\n",
        "    # Bá» qua cÃ¡c báº£n ghi dÃ¹ng mÃ¡y táº¡o nhá»‹p tim (Paced beats)\n",
        "    if record_name in ['102', '104', '107', '217']: continue\n",
        "\n",
        "    path = os.path.join('mitdb_data', record_name)\n",
        "    try:\n",
        "        record = wfdb.rdrecord(path)\n",
        "        annotation = wfdb.rdann(path, 'atr')\n",
        "        signal = record.p_signal[:, 0] # Láº¥y Lead II\n",
        "\n",
        "        # Resample vá» 125Hz\n",
        "        num_samples = int(len(signal) * TARGET_FS / record.fs)\n",
        "        signal = resample(signal, num_samples)\n",
        "        r_peaks = (annotation.sample * TARGET_FS / record.fs).astype(int)\n",
        "\n",
        "        for i, r_peak in enumerate(r_peaks):\n",
        "            label = annotation.symbol[i]\n",
        "            if label in aami_dict:\n",
        "                mapped = aami_dict[label]\n",
        "                if mapped in classes:\n",
        "                    cls_idx = class_map[mapped]\n",
        "\n",
        "                    # --- CHIáº¾N THUáº¬T AUGMENTATION ---\n",
        "                    # Class N (0): Chá»‰ láº¥y Ä‘Ãºng tÃ¢m (khÃ´ng nhÃ¢n báº£n Ä‘á»ƒ trÃ¡nh quÃ¡ nhiá»u)\n",
        "                    # Class S, V, F: Láº¥y tÃ¢m + Lá»‡ch trÃ¡i + Lá»‡ch pháº£i => NhÃ¢n 3 dá»¯ liá»‡u\n",
        "                    offsets = [0]\n",
        "                    if cls_idx != 0:\n",
        "                        offsets = [0, -SHIFT_RANGE, SHIFT_RANGE]\n",
        "\n",
        "                    for offset in offsets:\n",
        "                        center = r_peak + offset\n",
        "                        start, end = center - WINDOW_SIZE // 2, center + WINDOW_SIZE // 2\n",
        "                        if start >= 0 and end <= len(signal):\n",
        "                            seg = signal[start:end]\n",
        "                            data_by_class[cls_idx].append(seg)\n",
        "    except: pass\n",
        "\n",
        "# ==============================================================================\n",
        "# BÆ¯á»šC 3: CÃ‚N Báº°NG & CHUáº¨N HÃ“A\n",
        "# ==============================================================================\n",
        "X_final, y_final = [], []\n",
        "MAX_N = 30000 # TÄƒng giá»›i háº¡n máº«u N lÃªn Ä‘á»ƒ model há»c ká»¹ hÆ¡n\n",
        "\n",
        "print(\"\\nğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u sau Augmentation:\")\n",
        "for cls_idx, segments in data_by_class.items():\n",
        "    segments = np.array(segments)\n",
        "    count = len(segments)\n",
        "    print(f\" - Class {classes[cls_idx]}: {count} máº«u\")\n",
        "\n",
        "    # Giáº£m máº«u Class N náº¿u quÃ¡ nhiá»u\n",
        "    if cls_idx == 0 and count > MAX_N:\n",
        "        indices = np.random.choice(count, MAX_N, replace=False)\n",
        "        segments = segments[indices]\n",
        "\n",
        "    X_final.extend(segments)\n",
        "    y_final.extend([cls_idx] * len(segments))\n",
        "\n",
        "X_raw = np.array(X_final)\n",
        "y = np.array(y_final)\n",
        "\n",
        "# TÃ­nh Mean/Std toÃ n cá»¥c\n",
        "global_mean = np.mean(X_raw)\n",
        "global_std = np.std(X_raw)\n",
        "print(f\"\\nğŸ“ Global Scaler: Mean={global_mean:.4f}, Std={global_std:.4f}\")\n",
        "\n",
        "# LÆ°u file scaler_ecg.json Má»šI\n",
        "with open('scaler_ecg.json', 'w') as f:\n",
        "    json.dump({\"mean\": float(global_mean), \"std\": float(global_std)}, f)\n",
        "\n",
        "# Chuáº©n hÃ³a dá»¯ liá»‡u\n",
        "X = (X_raw - global_mean) / (global_std + 1e-6)\n",
        "X = X.reshape(-1, WINDOW_SIZE, 1)\n",
        "\n",
        "# Chia táº­p Train/Test (Test Ä‘á»ƒ riÃªng 15% Ä‘á»ƒ cháº¥m Ä‘iá»ƒm cuá»‘i cÃ¹ng)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y, random_state=42)\n",
        "\n",
        "# One-hot encoding (Báº¯t buá»™c cho Label Smoothing)\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# TÃ­nh trá»ng sá»‘ class\n",
        "class_weights = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))\n",
        "\n",
        "# ==============================================================================\n",
        "# BÆ¯á»šC 4: XÃ‚Y Dá»°NG MODEL (DEEP RESNET Vá»šI SWISH)\n",
        "# ==============================================================================\n",
        "def residual_block(x, filters, kernel_size=5, stride=1):\n",
        "    shortcut = x\n",
        "    # Layer 1\n",
        "    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('swish')(x)\n",
        "    x = SpatialDropout1D(0.1)(x) # SpatialDropout tá»‘t hÆ¡n Dropout thÆ°á»ng cho 1D\n",
        "\n",
        "    # Layer 2\n",
        "    x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Khá»›p shortcut náº¿u kÃ­ch thÆ°á»›c thay Ä‘á»•i\n",
        "    if stride > 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('swish')(x)\n",
        "    return x\n",
        "\n",
        "inp = Input(shape=(WINDOW_SIZE, 1))\n",
        "x = Conv1D(64, 7, padding='same')(inp)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('swish')(x)\n",
        "\n",
        "# Kiáº¿n trÃºc sÃ¢u dáº§n\n",
        "x = residual_block(x, 64, kernel_size=5)\n",
        "x = MaxPooling1D(2)(x)\n",
        "\n",
        "x = residual_block(x, 128, kernel_size=5)\n",
        "x = MaxPooling1D(2)(x)\n",
        "\n",
        "x = residual_block(x, 256, kernel_size=3)\n",
        "x = residual_block(x, 256, kernel_size=3)\n",
        "x = MaxPooling1D(2)(x)\n",
        "\n",
        "x = residual_block(x, 512, kernel_size=3)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "x = Dense(256, activation='swish')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inp, out)\n",
        "\n",
        "# ==============================================================================\n",
        "# BÆ¯á»šC 5: TRAINING (COSINE DECAY + LABEL SMOOTHING)\n",
        "# ==============================================================================\n",
        "# HÃ m giáº£m learning rate hÃ¬nh sin\n",
        "def cosine_decay(epoch):\n",
        "    lr_start = 0.001\n",
        "    lr_min = 0.00001\n",
        "    epochs = 60\n",
        "    return lr_min + 0.5 * (lr_start - lr_min) * (1 + np.cos(epoch / epochs * np.pi))\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(cosine_decay)\n",
        "checkpoint = ModelCheckpoint('best_ultra_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Label Smoothing = 0.1 giÃºp model khÃ¡i quÃ¡t hÃ³a tá»‘t hÆ¡n\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nğŸš€ Báº¯t Ä‘áº§u Training...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    epochs=60,\n",
        "    batch_size=128,\n",
        "    validation_split=0.15, # TrÃ­ch thÃªm 15% tá»« train Ä‘á»ƒ lÃ m validation\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# BÆ¯á»šC 6: ÄÃNH GIÃ & XUáº¤T FILE (Sá»¬A Lá»–I KERAS 3)\n",
        "# ==============================================================================\n",
        "print(\"\\nğŸ† ÄÃ¡nh giÃ¡ trÃªn táº­p Test (Dá»¯ liá»‡u chÆ°a tá»«ng há»c):\")\n",
        "model.load_weights('best_ultra_model.h5')\n",
        "\n",
        "# Dá»± Ä‘oÃ¡n\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "# BÃ¡o cÃ¡o chi tiáº¿t (4 chá»¯ sá»‘ tháº­p phÃ¢n)\n",
        "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
        "\n",
        "# Convert sang TFJS (CÃ¡ch má»›i cho Keras 3)\n",
        "print(\"\\nğŸ“¦ Äang Ä‘Ã³ng gÃ³i model...\")\n",
        "\n",
        "# 1. Export ra SavedModel (Thay vÃ¬ .h5)\n",
        "# Äiá»u nÃ y táº¡o ra má»™t thÆ° má»¥c 'saved_model_ecg' chá»©a cáº¥u trÃºc graph chuáº©n cá»§a TF\n",
        "model.export('saved_model_ecg')\n",
        "\n",
        "# 2. Sá»­ dá»¥ng tensorflowjs_converter vá»›i input_format=tf_saved_model\n",
        "# output_format=tfjs_graph_model lÃ  Ä‘á»‹nh dáº¡ng tá»‘i Æ°u nháº¥t cho web\n",
        "!tensorflowjs_converter \\\n",
        "    --input_format=tf_saved_model \\\n",
        "    --output_format=tfjs_graph_model \\\n",
        "    saved_model_ecg \\\n",
        "    ./tfjs_model\n",
        "\n",
        "# NÃ©n file Ä‘á»ƒ táº£i vá»\n",
        "!zip -r tfjs_model_ultra.zip tfjs_model scaler_ecg.json\n",
        "\n",
        "print(\"\\nâœ… HOÃ€N Táº¤T! HÃ£y táº£i file 'tfjs_model_ultra.zip' vá».\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cebq3fNN-e0"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}